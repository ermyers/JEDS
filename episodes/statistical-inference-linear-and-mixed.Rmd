---
title: "Statistical inference with general linear models and mixed models"
author: "Greg Maurer, Darren James"
date: '2022-06-19'
output:
    html_document:
      toc: true
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This lesson introduces statistical inference using a sequence of approaches
building from mean comparison procedures (t-tests), to simple linear models
(often called general linear models), through linear mixed models. The lesson
centers on the assumptions that underlie the process of fitting linear models.
These are:

1. The samples are drawn from a normal population, and therefore the residuals 
are normally distributed (not necessarily the whole dataset).
2. The populations the samples are drawn from have constant variance 
(they are *homoscedastic*).
3. The observations in each group, or sample, are independent of one another.

We should try to meet these assumptions when fitting linear models^[A fourth
assumption is that the continuous predictor variables, or covariates, are
measured without error, but that one is pretty tough to meet.], but when 
analyzing ecological data they are easily violated. So, as this lesson
introduces various types of statistical inference around linear models, we will 
also show ways to test these assumptions with our data, and fit alternative 
models (such as mixed models) when they can't be met.

### Our data and R packages

In addition to `tidyverse`, you will need to have the `car` package installed,
which adds some options for inspecting fitted regression models, such as type-II
and -III ANOVA output tables. You will also need `emmeans`, `multcomp`, and
`multcompView`for post-hoc comparisons of groups in fitted linear models, and
`lme4` and `lmerTest` to fit and test some mixed models.

Data for the exercises here come from the Jornada annual NPP data package on
EDI. You can learn more about this dataset on the [teaching datasets]('./teaching_datasets.Rmd#Teaching-dataset-1') page. To load the data we will first load the `tidyverse` and use its
`read_csv()` function to read in the data from EDI. Then we'll look at the
structure of this data.

```{r load_data, message=FALSE}
# Get tidyverse & the NPP data
library(tidyverse)

anpp <- read_csv('https://pasta.lternet.edu/package/data/eml/knb-lter-jrn/210011003/105/127124b0f04a1c71f34148e3d40a5c72')

str(anpp)
```

We have four columns: `year` is a number representing the year of observation, 
`zone` is a character representing one of the 5 major vegetation zones at the
Jornada, `site` is a string representing one of the 15 NPP study sites at the
Jornada, and `npp_g_m2` is our observed NPP value in $g\cdot m^{-2}$.

```{r}
# Then, make a summary figure
ggplot(anpp, aes(x = year, y = npp_g_m2, col = site, group = site)) +
  geom_line() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

## Comparing the mean of two samples (t-tests)

```{r, include=FALSE}
##
## Comparing two samples
##
```

We commonly need to compare two samples from a set of observations. In this
lesson, we examine net primary production data (NPP) collected at the Jornada.
This dataset has been collected for over 3 decades, and as shown above, there is
significant variability from year to year. Lets begin by comparing observations
in two years only, 2019 and 2020, to see if they are markedly different using a 
t-test.

First, create a dataframe that is subset of our full `anpp` dataframe.

```{r}
# Create a 2-year data subset
anpp.19.20 <- anpp %>%
  dplyr::filter(year %in% 2019:2020)
```

In t-tests we are usually testing whether the mean of two groups is different. 
Lets look at some summary statistics from our two years of data first.

```{r}
# Summary statistics
anpp.19.20.stats <- anpp.19.20 %>%
  group_by(year) %>%
  summarise(n = n(),
            mean = mean(npp_g_m2),
            std.dev = sd(npp_g_m2)) %>%
  mutate(std.err = std.dev/sqrt(n))

anpp.19.20.stats
```

From this we see that the means for each year are offset, increasing by about 
19 $g\cdot m^{-2}$ from 2019 to 2020. There are also differences in the
variance around the means, which we see as different standard deviation and
error statistics.

For a t-test we first need to put data into "wide" form, with observations
for the different years in separate columns.

```{r}
# Spread to wide form
anpp.19.20.wide <- anpp.19.20 %>%
  spread(year, npp_g_m2)

head(anpp.19.20.wide)
```

Now do a t-test for differences in mean NPP in 2019 and 2020, using the
`t.test()` function.

```{r}
# A default 2-sample t-test in R
t.test(x = anpp.19.20.wide$`2019`, 
       y = anpp.19.20.wide$`2020`)
```

We see that the default here is a "Welch Two Sample t-test", which is
an unequal variance test. That means that the test is designed to account for 
differences in variance between the 2019 and 2020 sample. We'll see later that
testing two samples with different variances is something we could also do 
with a mixed model. This test tells us that there isn't a "significant"
difference in the means since the p-value is greater than 0.05, which is our
$\alpha$ value, or the threshold we use to reject the null hypothesis that the
two means are the same.

Lets look at some other variations on the t-test. First, a standard Student's 
t-test with equal, or pooled, variance.

```{r}
# t-test with equal variance
t.test(x = anpp.19.20.wide$`2019`, 
       y = anpp.19.20.wide$`2020`, 
       var.equal = TRUE)
```

This lowers the p-value a tad, but is that a good thing? We've pooled the
variance for 2019 and 2020, even though we saw from our summary statistics that
the variances might be unequal, i.e., the standard deviation of the 2020
observations are higher.

Next lets try a paired t-test, which assumes that the observations in each
sample are not independent. We know this is the case since we're measuring the 
same sites each year, and therefore comparing two samples of the same 
experimental or statistical units (sites). In a paired t-test, the question
becomes whether the difference between the two samples, 2019 and 2020
observations, is itself different than zero. Therefore, if we already know that
we have unequal variances in our samples, taking the difference is a quick way
to get around that issue.

```{r}
# Paired t-test
t.test(x = anpp.19.20.wide$`2019`, 
       y = anpp.19.20.wide$`2020`, 
       paired = TRUE)
```

We have a significant result here (p-value < 0.05) which tells us that we can
reject the null hypothesis that the difference between these two years of data
is equal to zero. So, NPP changed from 2019 to 2020.

We'll see later that paired t-test can also be formulated as a type of linear
mixed model.

### How do we know which comparison (t-test) to use?

```{r, include=FALSE}
##
## How do we know which comparison (t-test) to use?
##
```

It can be challenging to know whether the data are paired, or whether they
have unequal variance. It is a good idea to be fairly systematic and examine
assumptions as you go. Lets take a slightly larger slice of our original dataset
and compare years 2017 and 2018 this time.

```{r}
# Create a slightly different data subset - 2017 and 2018
anpp.17.18 <- anpp %>%
  dplyr::filter(year %in% 2017:2018)
```

How would we test whether there are unequal variances within these two 1-year 
samples (i.e., whether the data are *heteroscedastic*) ? If we plot the data, we
will see some visual cues, and boxplots are a nice method for this.

```{r}
ggplot(anpp.17.18, aes(x = year, y = npp_g_m2, color = factor(year))) +
  geom_boxplot()
```

We see here that 2017 has much wider box, suggesting higher variance, than 2018.
We can use some additional tests to quantify that range in variance. The first
is the Bartlett test, which tests the null hypothesis that the variance in each
sample, or group, is equal.

```{r}
# Bartlett test for unequal variance (homescedasticity)
bartlett.test(npp_g_m2 ~ year, data = anpp.17.18)
```

The low p-value here suggestst that we don't have equal variance in 2017 and
2018. No big surprise given the boxplots we made.

However, perhaps our data are not normally distributed, in which case the 
Bartlett test is inadequate because it is sensitive to non-normal data. Lets
first check if the data are normal using the Shapiro-Wilks test, which tests the
null hypothesis that the data are normally distributed.

```{r}
# Test for normality of the data
shapiro.test(anpp.17.18$npp_g_m2)
```
We have a low p-value here, so we have strong evidence that we are actually
working with non-normally distributed data. Since that is the case, we might
want to test for unequal variances using one of two other tests, the Fligner
test or the Levene test.

```{r}
# Fligner test for homoscedasticity
fligner.test(npp_g_m2 ~ as.factor(year), data = anpp.17.18)
```
There is evidence for heteroscedasticity here. What about Levene?

```{r}
# Levene test for homoscedasticity, which is in the `car` R package
car::leveneTest(npp_g_m2 ~ as.factor(year), data = anpp.17.18)
```

Each of these tests appears to reject the null hypothesis that observations
occurring in these two years have the same variance.

Interesting, so it is beginning to look like the data are non-normal and have
unequal variance (are *heteroscedastic*). Are they independent? Well, as we know
from learning about the experimental design and inspecting the data, the same 
sites are measured year after year. This means the measurements in 2017 and 
2018 are not independent. So, to compare the means of these years we want to
do a paired t-test again.

```{r}
# Spread to wide form
anpp.17.18.wide <- anpp.17.18 %>%
  spread(year, npp_g_m2)

# Paired t-test
t.test(x = anpp.17.18.wide$`2017`, 
       y = anpp.17.18.wide$`2018`, 
       paired = TRUE,
       equal_variance=FALSE) # This is the default, we don't have to specify
```

We get a nice low p-value here, strong evidence that mean NPP changed from 2017
to 2018.

## Interlude - how should we interpret p-values?

An opinionated section on hypothesis testing with p-values.

## Comparing sample means with simple linear models

```{r, include=FALSE}
##
## Comparing sample means with simple linear models
##
```

Comparing the mean of different samples, as we have done with t-tests, is also
possible using simple linear models (sometimes referred to as *general linear
models*), a class of procedures often used in standard statistics that includes 
linear regression, one- and multi-way analysis of variance (ANOVA), and analysis
of covariance (ANCOVA). All of these are variations of "straight-line-fitting"
statistics, and `R` uses the `lm()`function for all these procedures. Instead of
comparing sample means directly, we are essentially comparing best-fit lines
drawn between the mean values of the samples.

Looking back at our 2019 and 2020 dataframe, lets compare some simple linear
models to the t-tests we did earlier in the lesson. A mean comparison test can
be done as a linear regression, using `lm` directly. We will fit a model and
then ask for summary results.

```{r}
# Two-sample mean comparison using a linear regression model
lm.ttest <- lm(npp_g_m2 ~ year, data = anpp.19.20)
summary(lm.ttest)
```

We get quite a few details on the linear model fit here, and see that we have
estimated the same difference in means and the same p-value as we had in the
earlier equal-variance two-sample t-test.

If we want to look at how this model is being fit visually, we can make a plot
using `ggplot`, which allows us to fitted models to a plot with `geom_smooth`.
We need to specify the type of model as "lm" to get a linear model.

```{r}
ggplot(data=anpp.19.20, aes(x=year, y=npp_g_m2)) +
  geom_point() +
  geom_smooth(method="lm")
```

Notice the difference in the means, represented by the slope of the line, and
the difference in the variance (vertical range of the points) between the 2019 
and 2020 samples.

We can also compare the means using one-way ANOVA. The function to do this is
`aov()`, but under the hood this is really just fitting an `lm` model again,
which we can see by looking at the function documentation.

```{r, message=FALSE}
?aov
```

One nuance here is that when given numerical predictor variables, like `year`,
`lm` interprets these as a continuous variable. For ANOVA, we aim to analyze
differences between discrete categories, so we should make sure we are using a
`factor`, which is how `R` represents categorical variables. Lets convert `year`
to a factor first.

```{r}
anpp.19.20.f <- anpp.19.20
anpp.19.20.f$year <- factor(anpp.19.20.f$year)
```

Now we can fit the ANOVA model using `aov()`, first specifying the linear model
and dataframe to use, and then ask R for summary statistics.

```{r}
# One-way ANOVA equivalent to the 2-sample t-test
aov.ttest <- aov(npp_g_m2 ~ year, data = anpp.19.20.f)
summary(aov.ttest)
# We could also use car::Anova here, but the results are the same
#car::Anova(lm.ttest, type = "III")
```

Notice that here, and when we used `lm`, our hypothesis test results were the 
same as with the unpaired, equal-variance, two sample t-test that we tried
earlier. This suggests that by default, `lm` is making some assumptions that
our observations are independent. We already know that they aren't. Later, we'll
see some ways to account for this.

When we have multiple categories in our data, we can do a multi-way ANOVA by 
adding additional factors to the model. In our case we have aother categorical
variable, called `zone`, that assigns each NPP site to one of the five major
vegetation zones at Jornada. Since this is a character column `lm`, or `aov`, 
will interpret that column as a factor, and we don't need to convert it first.

```{r}
# Two-way ANOVA using year and vegetation zone
aov.2way <- aov(npp_g_m2 ~ year + zone, data = anpp.19.20.f)
summary(aov.2way)
```

In our two-way ANOVA, we see that vegetation zone has a p-value below 0.05,
which suggests that we might be able to reject the null hypothesis that all the
vegetation zones have the same mean NPP value in 2019 and 2020.

This is all instructive, but we know that we should be using a paired model
since our observations are not independent. In ANOVA, the term for this is
*repeated measures*.  To use repeated measures with `aov` we must add an "Error"
term to our model that partitions the observed variance between years according
to `site`, since `site` is our experimental unit or subject.

```{r}
# One-way, repeated measures ANOVA using year and site
aov.1way.rm <- aov(npp_g_m2 ~ year + Error(site/year),
                   data = anpp.19.20.f)
summary(aov.1way.rm)
```

Somewhat unsurprisingly here, we have gotten the same result as with the earlier
paired t-test. This also suggests that we can use linear models to partition 
variance according to the groups in our data. Why would we want to do that?

## Comparing grouped data using linear models
```{r, include=FALSE}
##
## Comparing group means with ANOVA results
##
```

When we did a two-way ANOVA above, we saw some evidence that Jornada vegetation 
types, indicated by the `zone` variable in our dataset, have different mean 
values. Lets dig into this using the 2017 data and try compare the different 
zones. First lets subset the data and plot it by `zone`.

```{r}
# Subset to 2017
anpp.2017 <- anpp %>%
  dplyr::filter(year == "2017") %>%
  mutate(zone = factor(zone))

# Graph data by zone with boxplots
ggplot(anpp.2017, aes(x = zone, y = npp_g_m2, fill = zone)) +
  geom_boxplot()
```

We see several things here, including some we noticed earlier. Some vegetation
zones, like `P` (for Playa grassland), have high variance, and some, like `T` 
(for Tarbush shrubland), have low variance. We also see some pretty big 
differences in observed NPP between the zones.

We can use a linear model to tell us about differences between those groups. As
we do that, we should again check some of our linear model assumptions. Lets fit
the model using `lm`. Since `zone` is a factor this will be treated as an ANOVA.

```{r}
# Run ANOVA with lm()
lm.ANOVA <- lm(npp_g_m2 ~ zone, data = anpp.2017)
```

Lets get our ANOVA summary results two ways. First the standard `R` way, which
gives Type 1 sums of squares.

```{r}
summary(lm.ANOVA)
```

And now lets use the `car` way, which gives Type III sums of squares (Darren
recommends this).

```{r}
car::Anova(lm.ANOVA, type = "III", test.statistic = "F")
```
In this case the results are the same with both types of sums of squares.

The results above are interesting, and suggest that mean NPP is different 
between the vegetation zones. We have more interpretation of this ahead, but
first we should look a little more closely at how this model is meeting linear 
model assumptions. `R` gives us some nice ways to evaluate fitted `lm` models,
including information about the normality of the residuals. These are accessed by
plotting the fitted model itself using `plot()`.

```{r, fig.height=6}
# Diagnostic plots - here we put them all on one page
par(mfrow = c(2, 2), oma = c(0, 0, 2, 0)) -> opar
plot(lm.ANOVA)
```

The top right figure is key here, and the divergence from a 1:1 line suggests
that the residuals from the model are not completely normal. Lets test this
directly using the Shapiro-Wilk test.

```{r}
# Test for normality of residuals
resid(lm.ANOVA) %>% shapiro.test()
```
Actually its not too bad. The residuals seem to be "normal enough"!

### Post-hoc tests and pairwise comparisons

```{r, include=FALSE}
##
## Post-hoc tests and pairwise comparisons
##
```

The ANOVA we did for the 2017 data gave us some useful information. We can see
that from the `zone` coefficients and p-values that vegetation type has a large
effect on the mean NPP values. That is useful information, but the ANOVA summary
table isn't enough to determine which vegetation zones have means that are 
*different than each other*.

Making comparisons between groups is typically something we do after fitting an
ANOVA model, and is called a *post-hoc* comparison or test^[As with other
statistical procedures, post-hoc comparisons have other names, like *pairwise comparisons* or *multiple comparisons*.]. The `multcomp` and `emmeans` packages
are commonly used for these types of comparisons. 

The first step, after loading `emmeans` is to examine the least squares means
given by our ANOVA model for each vegetation zone.

```{r}
library(emmeans)
# Least squares means (estimated marginal means)
emmeans(lm.ANOVA, ~ zone)
```

This gives an estimated mean and upper and lower 95% confidence levels. We can
do some pairwise comparisons of these means, comparing each zone to the four 
other zones to determine which are "significantly different". Note that the
formula for the number if independent comparisons is always $n(n-1)/2$, where
$n$ is the number of groups.

```{r}
# Pairwise comparisons of least squares means
contrast(emmeans(lm.ANOVA, specs= ~zone), method = "pairwise")
```
This gives us ten mean comparisons using the Tukey method, and in a few cases,
the p-values indicate that we can reject the null hypothesis that the means
are the same^[This should be equivalent to using `TukeyHSD()` with an `aov` 
object, if you are used to that.]. But, it would be nice to put these together in
a way that tells us which groups are  statistically different, and which are
not. We do that by adding the pairwise comparison letters with `multcomp::cld`.
The default comparison here is, again, the Tukey method.

```{r}
# Letter separations (efficient way to display)
emmeans.Tukey <- emmeans(lm.ANOVA, ~ zone) %>%
  multcomp::cld(Letters = LETTERS)

emmeans.Tukey
```
This groups the comparisons, telling us that the group `P`, or Playa grasslands
are significantly more productive in 2017, while the other four groups are
statistically the same.

```{r, include=FALSE}
# Problem: emmeans switches to Sidak when adjust = "Tukey" is specified
# I actually found this from Russ Lenth:
#    https://stats.stackexchange.com/questions/508055/unclear-why-adjust-tukey-was-changed-to-sidak
# It seems the confidence limits are being adjusted when you specify adjust = 
# "Tukey" but not the means themselves. This is intended I guess, but I'm still
# fuzzy on the statistical meaning.
emmeans(lm.ANOVA, ~ zone, adjust = "Tukey") %>%
  multcomp::cld(Letters = LETTERS)
```

We've based these mean comparison letters on the default Tukey method and its
confidence intervals. However, there are other methods that might be more or
less conservative. Lets compare results from Tukey and Scheffe methods, by 
re-doing the comparison, binding the tables, and making a figure.

```{r}
# First get comparisons from Scheffe
emmeans.Scheffe <- emmeans(lm.ANOVA, specs = ~ zone) %>% 
  multcomp::cld(Letters = LETTERS, adjust = "scheffe") %>%
  as_tibble() %>%
  mutate(method = "Scheffe")

# Put the Tukey results in a tibble
emmeans.Tukey <- emmeans.Tukey %>% as_tibble() %>% mutate(method = "Tukey")

# Look at least squares means and 95% confidence intervals for
# Tukey and Scheffe
bind_rows(emmeans.Scheffe, emmeans.Tukey) %>%
  mutate(.group = trimws(.group)) %>%
  ggplot(aes(x = zone, y = emmean, col = zone)) +
  geom_point() +
  geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL), width=.1) +
  geom_text(aes(x = zone, y = emmean, label = .group),
            hjust = -0.5, show.legend = FALSE) +
  facet_wrap(~ method, ncol = 1) +
  ggtitle("2017 ANPP by Vegetation Zone", 
          subtitle = "Same-letter means are not different at alpha = 0.05") +
  ylab("Model-based mean ANPP (g/m^2) +/- 95% CI")
```

They are similar but not the same, and you can see that the error bars, here
showing the 95% confidence intervals estimated for each method, give some
indication of how they are grouping the comparisons. In scientific papers, the
convention is to plot figures like this with standard error bars. Lets take a
look at the difference.

```{r}
# Look at least squares means and standard errors
# This kind of figure is the one most commonly reported in scientific papers
bind_rows(emmeans.Tukey, emmeans.Scheffe) %>%
  mutate(.group = trimws(.group)) %>%
  ggplot(aes(x = zone, y = emmean, col = zone)) +
  geom_point() +
  geom_errorbar(aes(ymin=emmean-SE, ymax=emmean+SE), width=.1) +
  geom_text(aes(x = zone, y = emmean, label = .group), 
            hjust = - 0.5, show.legend = FALSE) +
  facet_wrap(~ method, ncol = 1) +
  ggtitle("2017 ANPP by Vegetation Zone", 
          subtitle = "Same-letter means are not different at alpha = 0.05") +
  ylab("Model-based mean ANPP (g/m^2) +/- S.E.")
```

Notice that if we were basing our comparisons on standard error bar overlaps
alone, we would draw a different conclusion about which vegetation zones had
different means.

## Into the wilderness with linear mixed models

```{r, include=FALSE}
##
## Into the wilderness with linear mixed models
##
```

In the last section we learned to compare the means across all the groups in our
dataset. We were using a very restricted dataset of one year, so there was no
issue with independence, but we've already seen that we need approaches like
paired t-tests and repeated measures ANOVA when observations aren't independent.
We also ignored the somewhat unequal variance among groups in our observations,
even though we know that it exists. As we look at additional years of data,
we will need to reconsider both of these assumptions when fitting linear models.

Mixed models help us get around two of the main assumptions behind linear 
models. In particular they are useful for fitting linear models in cases where:

1. there is unequal variance between groups in the data
2. the observations or samples are not independent, like we often see in an
ecological time series

Mixed models are potentially very complex, and there is not perfect agreement in
the statistics community on the best way to fit them. Additionally, in `R` there
are more than the usual number of approaches and packages for fitting mixed
models. Prepare to be a little confused sometimes, and to do your own research.
Here, we will introduce some basics.

There are two primary `R` packages for linear mixed modeling. `nlme` and `lme4`.
Lets load them.

```{r, message = FALSE}
library(nlme)
library(lme4)
```

To get started, lets look at a four-year NPP dataset- 2017 to 2020. First we'll
subset and plot the dataset.

```{r}
# Subset the data between 2017 and 2020
anpp.17.20 <- anpp %>%
  dplyr::filter(year %in% 2017:2020) 

# Plot the data
ggplot(anpp.17.20, aes(x = year, y = npp_g_m2, col = zone, group = site)) +
  geom_point() +
  geom_line(linetype="dashed", size = .05) +
  scale_x_continuous(breaks = 2017:2020)
```

Some summary statistics will show us that these data have unequal variance,
in particular higher variance in 2017.

```{r}
# Variance is highest in 2017
anpp.17.20 %>%
  group_by(year) %>%
  summarise(std.dev = sd(npp_g_m2))
```

Since we have a time series of non-independent measurements, and unequal 
variance in our samples, we should account for this when we fit a linear model.
We'll begin by fitting a repeated measures model with `gls` from the `nlme`
package. 

```{r}
# Fit a repeated measures model (unstructured correlated errors)
model.un <- gls(npp_g_m2 ~ zone*factor(year),
                corSymm(form = ~ 1 | site),
                weights=varIdent(form=~1|factor(year)),
                data = anpp.17.20)
```

The model above has specifies and unstructured correlation between error terms.
Since this is a time series, we can also reasonably assume that each observation
at a site is correlated with previous observations at the site, which is
something we can also specify in this model. Lets do that and compare the fitted
models.

```{r}
# Fit a repeated measures model (correlated errors with autoregressive
# covariance structure)
model.ar <- gls(npp_g_m2 ~ zone*factor(year),  
                corAR1(form = ~ 1 | site), 
                weights=varIdent(form=~1|factor(year)), 
                data = anpp.17.20)
```

Now we'll compare these two models using the Akaike Information Criterion, or
AIC (the function in `R` is `AIC()`). Comparing models this way is part of the
information theoretic approach to model selection. A lower AIC value for the
same data, indicates a better-fitting, and more parsimonious model.

```{r}
AIC(model.un)
AIC(model.ar)
```
We can see that the model with an autoregressive covariance structure is a
"better" model.

To improve our model further, we could break the data into high variance and
low variance groups (called `level`), since we know there is unequal variance
during different years. We'll assign 2017 to a high variance group, and the
other years to a low variance group, and then fit a model that includes a random effect of the `level` variance group. We'll also re-calculate the AIC value.

```{r}
# Assign 2017 to high variance group, other years to low
anpp.17.20 <- anpp.17.20 %>%
  mutate(anpp_level = if_else(year == 2017, "high", "low")) 

# Re-fit a model with a variance level
model.ar <- gls(npp_g_m2 ~ zone*factor(year),  
                      corAR1(form = ~ 1 | site), 
                      weights=varIdent(form=~1|anpp_level), 
                      data = anpp.17.20)

AIC(model.ar)
```
The AIC value dropped, which suggests that grouping by the new `level` category
is improving the fit of this model. Now we can compare the estimated means
from this mixed model using `emmeans` again.

```{r}
emmeans(model.ar, ~ zone*factor(year), lmer.df = "kenward-roger") 
```

It is a little more difficult to extract correlation and variance estimates
```{r}
# Bet correlation and variance estimates
coef(model.un$modelStruct$corStruct, uncons = FALSE, allCoef = TRUE)
coef(model.un$modelStruct$varStruct, uncons = FALSE, allCoef = TRUE) 
```

Now lets compare the fit of three different mixed models that are specified
similarly, but using 2 different functions from `nlme`, and one from `lme4`.
Take note of how the models are specified differently in the arguments to each 
function.

```{r}
# R-side model from nlme::gls()
model.ar.level.gls <- gls(npp_g_m2 ~ zone*factor(year),  
                          corAR1(form = ~ 1 | site), 
                          weights=varIdent(form=~1|anpp_level), 
                          data = anpp.17.20,
                          method = 'REML')

# G-side model from nlme::lme()
model.ar.level.lme <- lme(fixed = npp_g_m2 ~ zone*factor(year),  
                          random = ~ 1 | site,
                          weights = varIdent(form = ~ 1 | anpp_level),
                          data = anpp.17.20,
                          method = 'REML')

# G-side model from lme4::lmer()
model.ar.level.lmer <- lmer(npp_g_m2 ~ zone*factor(year) + 
                              (anpp_level | site),
                            data = anpp.17.20 %>%
                              mutate(anpp_level = factor(anpp_level),
                                     site = factor(site)))

AIC(model.ar.level.gls)
AIC(model.ar.level.lme)
AIC(model.ar.level.lmer)
```

```{r}
mod.compare <- emmeans(model.ar.level.gls, ~zone*factor(year),
                       lmer.df = "satterthwaite") %>%
  multcomp::cld(alpha = 0.05, Letters = LETTERS) %>%
  mutate(model = "gls") %>%
  bind_rows(emmeans(model.ar.level.lme, ~zone*factor(year),
                    lmer.df = "satterthwaite") %>%
              multcomp::cld(alpha = 0.05, Letters = LETTERS) %>%
              mutate(model = "lme")) %>%
  bind_rows(emmeans(model.ar.level.lmer, ~zone*factor(year),
                    lmer.df = "satterthwaite") %>%
              multcomp::cld(alpha = 0.05, Letters = LETTERS) %>%
              mutate(model = "lmer"))

# Compare means ... should all be the same
mod.compare %>%
  dplyr::select(zone, year, emmean, model) %>%
  spread(model, emmean)

# Compare standard errors
mod.compare %>%
  dplyr::select(zone, year, SE, model) %>%
  spread(model, SE)

# Compare mean comparison results
mod.compare %>%
  dplyr::select(zone, year, .group, model) %>%
  spread(model, .group)

pd <- position_dodge(0.5)
ggplot(mod.compare, aes(x = year, y = emmean, group = zone, colour = zone)) +
  geom_point(position=pd) +
  geom_line(position=pd) +
  geom_errorbar(aes(ymin = emmean - SE , ymax = emmean + SE), width=0.2, position=pd) +
  geom_text(aes(label = .group, y = emmean), 
            position = pd, hjust = -0.1, col = "black", size = 2.5) +
  facet_wrap(~ model, ncol = 1, strip.position = "right") +
  theme(legend.position = "top")

```